import React, { useState, useEffect, useCallback, memo } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '../ui/card';
import { Button } from '../ui/button';
import { Badge } from '../ui/badge';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '../ui/select';
import { Alert, AlertDescription } from '../ui/alert';
import { Textarea } from '../ui/textarea';
import { toast } from 'sonner@2.0.3';

// AI Model Configuration
interface AIModel {
  id: string;
  name: string;
  provider: string;
  capabilities: string[];
  maxTokens: number;
  costPerToken: number;
  speed: 'fast' | 'medium' | 'slow';
  quality: 'high' | 'medium' | 'standard';
  status: 'available' | 'limited' | 'offline';
}

interface AIResponse {
  content: string;
  model: string;
  tokensUsed: number;
  responseTime: number;
  confidence: number;
}

interface ConversationContext {
  messages: Array<{
    role: 'user' | 'assistant' | 'system';
    content: string;
    model?: string;
    timestamp: number;
  }>;
  projectContext?: any;
  codeContext?: string[];
}

const AI_MODELS: AIModel[] = [
  {
    id: 'gpt-4-turbo',
    name: 'GPT-4 Turbo',
    provider: 'OpenAI',
    capabilities: ['code', 'analysis', 'creative', 'reasoning'],
    maxTokens: 128000,
    costPerToken: 0.00001,
    speed: 'fast',
    quality: 'high',
    status: 'available'
  },
  {
    id: 'claude-3-opus',
    name: 'Claude 3 Opus',
    provider: 'Anthropic',
    capabilities: ['code', 'analysis', 'creative', 'reasoning', 'safety'],
    maxTokens: 200000,
    costPerToken: 0.000015,
    speed: 'medium',
    quality: 'high',
    status: 'available'
  },
  {
    id: 'gemini-pro',
    name: 'Gemini Pro',
    provider: 'Google',
    capabilities: ['code', 'analysis', 'multimodal', 'reasoning'],
    maxTokens: 32000,
    costPerToken: 0.000005,
    speed: 'fast',
    quality: 'high',
    status: 'available'
  },
  {
    id: 'codellama-34b',
    name: 'CodeLlama 34B',
    provider: 'Meta',
    capabilities: ['code', 'debugging', 'optimization'],
    maxTokens: 16000,
    costPerToken: 0.000002,
    speed: 'fast',
    quality: 'medium',
    status: 'available'
  },
  {
    id: 'mistral-large',
    name: 'Mistral Large',
    provider: 'Mistral AI',
    capabilities: ['code', 'analysis', 'reasoning'],
    maxTokens: 32000,
    costPerToken: 0.000008,
    speed: 'fast',
    quality: 'high',
    status: 'available'
  }
];

const MultiModelAIService: React.FC = memo(() => {
  const [selectedModel, setSelectedModel] = useState<string>('gpt-4-turbo');
  const [prompt, setPrompt] = useState<string>('');
  const [isGenerating, setIsGenerating] = useState<boolean>(false);
  const [responses, setResponses] = useState<AIResponse[]>([]);
  const [conversationContext, setConversationContext] = useState<ConversationContext>({
    messages: []
  });
  const [autoModelSelection, setAutoModelSelection] = useState<boolean>(true);
  const [costEstimate, setCostEstimate] = useState<number>(0);

  // Calculate cost estimate based on prompt length
  useEffect(() => {
    const model = AI_MODELS.find(m => m.id === selectedModel);
    if (model && prompt) {
      const estimatedTokens = Math.ceil(prompt.length / 4); // Rough token estimation
      setCostEstimate(estimatedTokens * model.costPerToken);
    } else {
      setCostEstimate(0);
    }
  }, [selectedModel, prompt]);

  // Auto-select best model based on task
  const selectOptimalModel = useCallback((taskPrompt: string): string => {
    const lowerPrompt = taskPrompt.toLowerCase();
    
    if (lowerPrompt.includes('code') || lowerPrompt.includes('debug') || lowerPrompt.includes('function')) {
      return 'codellama-34b'; // Specialized for coding
    }
    
    if (lowerPrompt.includes('analyze') || lowerPrompt.includes('review') || lowerPrompt.includes('explain')) {
      return 'claude-3-opus'; // Best for analysis
    }
    
    if (lowerPrompt.includes('creative') || lowerPrompt.includes('design') || lowerPrompt.includes('story')) {
      return 'gpt-4-turbo'; // Best for creative tasks
    }
    
    if (lowerPrompt.includes('multimodal') || lowerPrompt.includes('image') || lowerPrompt.includes('visual')) {
      return 'gemini-pro'; // Multimodal capabilities
    }
    
    // Default to fastest, most cost-effective
    return AI_MODELS.sort((a, b) => 
      (a.costPerToken * (a.speed === 'fast' ? 1 : 2)) - 
      (b.costPerToken * (b.speed === 'fast' ? 1 : 2))
    )[0].id;
  }, []);

  // Generate response with fallback mechanism
  const generateResponse = useCallback(async (useModel?: string) => {
    if (!prompt.trim()) {
      toast.error('Please enter a prompt');
      return;
    }

    setIsGenerating(true);
    const startTime = Date.now();
    
    let modelToUse = useModel || selectedModel;
    
    // Auto-select optimal model if enabled
    if (autoModelSelection && !useModel) {
      modelToUse = selectOptimalModel(prompt);
      setSelectedModel(modelToUse);
    }

    try {
      // Call the AI service through Supabase Edge Function
      const response = await fetch(`${import.meta.env.VITE_SUPABASE_URL}/functions/v1/make-server-88829a40/ai/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_ANON_KEY}`
        },
        body: JSON.stringify({
          model: modelToUse,
          prompt,
          context: conversationContext,
          maxTokens: AI_MODELS.find(m => m.id === modelToUse)?.maxTokens || 4000
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();
      const responseTime = Date.now() - startTime;

      const aiResponse: AIResponse = {
        content: data.content,
        model: modelToUse,
        tokensUsed: data.tokensUsed || 0,
        responseTime,
        confidence: data.confidence || 0.95
      };

      setResponses(prev => [aiResponse, ...prev]);
      
      // Update conversation context
      setConversationContext(prev => ({
        ...prev,
        messages: [
          ...prev.messages,
          { role: 'user', content: prompt, timestamp: Date.now() },
          { role: 'assistant', content: data.content, model: modelToUse, timestamp: Date.now() }
        ]
      }));

      toast.success(`Response generated with ${AI_MODELS.find(m => m.id === modelToUse)?.name}`);
      setPrompt(''); // Clear prompt after successful generation

    } catch (error) {
      console.error('AI generation failed:', error);
      
      // Fallback mechanism - try with different model
      const fallbackModels = AI_MODELS.filter(m => m.id !== modelToUse && m.status === 'available');
      
      if (fallbackModels.length > 0) {
        const fallbackModel = fallbackModels[0];
        toast.warning(`${AI_MODELS.find(m => m.id === modelToUse)?.name} failed, trying ${fallbackModel.name}...`);
        await generateResponse(fallbackModel.id);
        return;
      }

      toast.error(`AI generation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    } finally {
      setIsGenerating(false);
    }
  }, [prompt, selectedModel, conversationContext, autoModelSelection, selectOptimalModel]);

  // Compare responses across multiple models
  const compareModels = useCallback(async () => {
    if (!prompt.trim()) {
      toast.error('Please enter a prompt to compare models');
      return;
    }

    setIsGenerating(true);
    
    const activeModels = AI_MODELS.filter(m => m.status === 'available').slice(0, 3); // Limit to 3 for cost
    
    try {
      const promises = activeModels.map(async (model) => {
        const startTime = Date.now();
        
        const response = await fetch(`${import.meta.env.VITE_SUPABASE_URL}/functions/v1/make-server-88829a40/ai/generate`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_ANON_KEY}`
          },
          body: JSON.stringify({
            model: model.id,
            prompt,
            context: conversationContext,
            maxTokens: Math.min(model.maxTokens, 1000) // Limit tokens for comparison
          })
        });

        if (!response.ok) {
          throw new Error(`${model.name} failed: ${response.statusText}`);
        }

        const data = await response.json();
        const responseTime = Date.now() - startTime;

        return {
          content: data.content,
          model: model.id,
          tokensUsed: data.tokensUsed || 0,
          responseTime,
          confidence: data.confidence || 0.95
        };
      });

      const results = await Promise.allSettled(promises);
      const successfulResponses = results
        .filter((result): result is PromiseFulfilledResult<AIResponse> => result.status === 'fulfilled')
        .map(result => result.value);

      setResponses(prev => [...successfulResponses, ...prev]);
      toast.success(`Compared ${successfulResponses.length} models successfully`);

    } catch (error) {
      console.error('Model comparison failed:', error);
      toast.error('Model comparison failed');
    } finally {
      setIsGenerating(false);
    }
  }, [prompt, conversationContext]);

  return (
    <div className="space-y-6 max-w-6xl mx-auto">
      {/* Header */}
      <div className="text-center space-y-4">
        <h1 className="text-3xl font-bold bg-gradient-to-r from-primary via-secondary to-accent bg-clip-text text-transparent font-sora">
          Multi-Model AI Integration
        </h1>
        <p className="text-muted-foreground max-w-2xl mx-auto">
          Access multiple AI models with intelligent routing, fallback mechanisms, and cost optimization
        </p>
      </div>

      {/* Model Selection & Configuration */}
      <div className="grid lg:grid-cols-3 gap-6">
        <Card className="lg:col-span-2">
          <CardHeader>
            <CardTitle className="flex items-center space-x-2">
              <span>ü§ñ</span>
              <span>AI Model Configuration</span>
            </CardTitle>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="grid md:grid-cols-2 gap-4">
              <div>
                <label className="text-sm font-medium mb-2 block">Selected Model</label>
                <Select value={selectedModel} onValueChange={setSelectedModel}>
                  <SelectTrigger>
                    <SelectValue />
                  </SelectTrigger>
                  <SelectContent>
                    {AI_MODELS.map((model) => (
                      <SelectItem key={model.id} value={model.id}>
                        <div className="flex items-center justify-between w-full">
                          <span>{model.name}</span>
                          <div className="flex items-center space-x-2 ml-4">
                            <Badge variant={model.status === 'available' ? 'default' : 'secondary'} className="text-xs">
                              {model.status}
                            </Badge>
                            <Badge variant="outline" className="text-xs">
                              {model.provider}
                            </Badge>
                          </div>
                        </div>
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
              </div>

              <div className="flex items-center space-x-4">
                <div className="flex items-center space-x-2">
                  <input
                    type="checkbox"
                    id="auto-select"
                    checked={autoModelSelection}
                    onChange={(e) => setAutoModelSelection(e.target.checked)}
                    className="rounded"
                  />
                  <label htmlFor="auto-select" className="text-sm font-medium">
                    Auto-select optimal model
                  </label>
                </div>
              </div>
            </div>

            {/* Model Details */}
            {selectedModel && (
              <div className="p-4 bg-muted/30 rounded-lg">
                {(() => {
                  const model = AI_MODELS.find(m => m.id === selectedModel);
                  if (!model) return null;
                  
                  return (
                    <div className="grid md:grid-cols-2 lg:grid-cols-4 gap-4 text-sm">
                      <div>
                        <span className="font-medium">Max Tokens:</span>
                        <p className="text-muted-foreground">{model.maxTokens.toLocaleString()}</p>
                      </div>
                      <div>
                        <span className="font-medium">Speed:</span>
                        <p className="text-muted-foreground capitalize">{model.speed}</p>
                      </div>
                      <div>
                        <span className="font-medium">Quality:</span>
                        <p className="text-muted-foreground capitalize">{model.quality}</p>
                      </div>
                      <div>
                        <span className="font-medium">Est. Cost:</span>
                        <p className="text-muted-foreground">${costEstimate.toFixed(6)}</p>
                      </div>
                    </div>
                  );
                })()}
              </div>
            )}

            {/* Prompt Input */}
            <div>
              <label className="text-sm font-medium mb-2 block">Prompt</label>
              <Textarea
                value={prompt}
                onChange={(e) => setPrompt(e.target.value)}
                placeholder="Enter your prompt here... FlashFusion will automatically select the best model or use your chosen one."
                rows={4}
                className="resize-none"
              />
            </div>

            {/* Action Buttons */}
            <div className="flex flex-wrap gap-3">
              <Button
                onClick={() => generateResponse()}
                disabled={isGenerating || !prompt.trim()}
                className="bg-gradient-to-r from-primary to-primary/90"
              >
                {isGenerating ? (
                  <div className="flex items-center space-x-2">
                    <div className="w-4 h-4 border-2 border-white border-t-transparent rounded-full animate-spin"></div>
                    <span>Generating...</span>
                  </div>
                ) : (
                  <>
                    <span className="mr-2">‚ö°</span>
                    Generate Response
                  </>
                )}
              </Button>

              <Button
                onClick={compareModels}
                disabled={isGenerating || !prompt.trim()}
                variant="outline"
              >
                <span className="mr-2">üîÑ</span>
                Compare Models
              </Button>

              <Button
                onClick={() => {
                  setResponses([]);
                  setConversationContext({ messages: [] });
                }}
                variant="outline"
              >
                <span className="mr-2">üóëÔ∏è</span>
                Clear History
              </Button>
            </div>
          </CardContent>
        </Card>

        {/* Model Statistics */}
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center space-x-2">
              <span>üìä</span>
              <span>Model Stats</span>
            </CardTitle>
          </CardHeader>
          <CardContent>
            <div className="space-y-4">
              {AI_MODELS.map((model) => (
                <div key={model.id} className="flex items-center justify-between p-3 rounded-lg bg-muted/30">
                  <div>
                    <h4 className="font-medium text-sm">{model.name}</h4>
                    <p className="text-xs text-muted-foreground">{model.provider}</p>
                  </div>
                  <div className="text-right">
                    <div className="flex items-center space-x-1 mb-1">
                      <Badge
                        variant={model.status === 'available' ? 'default' : 'secondary'}
                        className="text-xs"
                      >
                        {model.status}
                      </Badge>
                    </div>
                    <div className="text-xs text-muted-foreground">
                      {model.capabilities.slice(0, 2).join(', ')}
                    </div>
                  </div>
                </div>
              ))}
            </div>
          </CardContent>
        </Card>
      </div>

      {/* Responses */}
      {responses.length > 0 && (
        <div className="space-y-4">
          <h2 className="text-xl font-semibold">Generated Responses</h2>
          
          <div className="grid gap-4">
            {responses.map((response, index) => {
              const model = AI_MODELS.find(m => m.id === response.model);
              
              return (
                <Card key={index} className="relative">
                  <CardHeader className="pb-3">
                    <div className="flex items-center justify-between">
                      <div className="flex items-center space-x-3">
                        <Badge className="bg-gradient-to-r from-primary/20 to-secondary/20">
                          {model?.name || response.model}
                        </Badge>
                        <span className="text-sm text-muted-foreground">
                          {response.responseTime}ms
                        </span>
                        <span className="text-sm text-muted-foreground">
                          {response.tokensUsed} tokens
                        </span>
                      </div>
                      <div className="flex items-center space-x-2">
                        <div className="text-xs text-muted-foreground">
                          Confidence: {(response.confidence * 100).toFixed(1)}%
                        </div>
                        <Button size="sm" variant="outline" onClick={() => {
                          navigator.clipboard.writeText(response.content);
                          toast.success('Response copied to clipboard');
                        }}>
                          Copy
                        </Button>
                      </div>
                    </div>
                  </CardHeader>
                  <CardContent>
                    <div className="prose prose-sm max-w-none dark:prose-invert">
                      <pre className="whitespace-pre-wrap text-sm leading-relaxed">
                        {response.content}
                      </pre>
                    </div>
                  </CardContent>
                </Card>
              );
            })}
          </div>
        </div>
      )}

      {/* Usage Alert */}
      <Alert>
        <AlertDescription>
          üí° <strong>Pro Tip:</strong> Enable auto-model selection for optimal performance and cost efficiency. 
          FlashFusion automatically chooses the best model based on your task type and requirements.
        </AlertDescription>
      </Alert>
    </div>
  );
});

MultiModelAIService.displayName = 'MultiModelAIService';

export default MultiModelAIService;